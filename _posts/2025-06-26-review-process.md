---
layout: post
title:  "Reflections on the FAccT 2025 Review Process"
author: "2025 Program Chairs (Jenn Wortman Vaughan, Shakir Mohamed, Sina Fazelpour, and Talia B. Gillis)"
comments: true
tags: reviewing facct-2025
excerpt_separator: <!--more-->
sticky: false
hidden: false
---

We are here in Athens this week for the 8th annual ACM Conference on Fairness, Accountability, and Transparency. As we enjoy the presentations, we want to take a moment to reflect on our year as Program Chairs, the FAccT review process, and our recommendations for future conference organizers.

<!--more-->

FAccT is an interdisciplinary conference by design, dedicated to bringing together a diverse community of scholars to advance research in responsible, safe, ethical, and trustworthy computing. The breadth of expertise and perspectives within our community is what makes FAccT both unique and intellectually rich. At the same time, organizing a review process that is cohesive and inclusive across disciplines remains a challenge that we as a community face each year. 

As described in our [first blog post](https://facct-blog.github.io/2024-10-28/announce-cfp), our goals as Program Chairs were to reinforce the inclusive “big tent” vision laid out in the FAccT strategic plan, maintain FAccT’s position as a top-tier publication venue, standardize and document the paper review process and related materials, and increase the degree of engagement and transparency with the community. We kept these goals in mind in all of our decision making. For instance, we broadened the range of topics included in the Call for Papers, recruited Reviewers and Area Chairs (ACs) with the diverse expertise necessary to review research across fields, and developed training material to improve consistency in the review process. 

We are delighted with the set of papers that were selected to be presented at the conference and appear in the proceedings, while also recognizing that the conference must continue to adapt and improve.

This year FAccT received a record 812 full paper submissions, up 12% from the 725 submissions in 2024. Reviewing this many papers required the collective effort of almost 700 reviewers and 117 ACs. In total, [218 papers](https://facctconference.org/2025/acceptedpapers) were accepted for publication, for an acceptance rate of 26.8%, up from 24.1% in 2024. We view this increase as a reflection of the high quality of paper submissions. Of these accepted papers, 206 papers are included in the proceedings, while the authors of 11 opted to include an abstract only. One paper was withdrawn after acceptance.

There are several aspects of the review process we believe went well:

- **Extended review timeline**: In 2024, FAccT introduced a rebuttal period of 48 hours for authors to respond to their reviews.  Recognizing the value of the rebuttal phase, we increased the length of the rebuttal period to six days to give authors more time and flexibility to reflect on their reviews. We also added a buffer into each main stage of the review process — from assignment of initial reviewers to the submission of initial reviews to submission of revised reviews, meta-reviews, and decision recommendations. Even with the extended timeline, we still faced challenges tracking down late reviews and encouraging Reviewers’ and ACs’ engagement in discussion threads.

- **Detailed guides and frequent communication**: As mentioned above, we developed detailed guides for [authors](https://facctconference.org/2025/aguide), [ACs](https://facctconference.org/2025/acguide), and [Reviewers](https://facctconference.org/2025/rguide), explaining their roles and responsibilities and answering common questions. These guides served both as source-of-truth references on policies and as an accessible introduction to the structure and expectations of FAccT’s review process since this style of review is typical of computer science venues but less common in the other disciplines represented at FAccT. We encourage future Program Chairs to maintain and update these guides. To supplement these guides, we also sent out frequent updates to ACs and Reviewers during the review process, explaining the current stage of the process and their role in it, and letting them know what to expect next.

- **Calibration process**: When making final decisions about which papers to accept or reject, our general approach was to rely on the expertise of our ACs and Reviewers. When there was unanimous agreement among the review team assigned to a paper on whether or not the paper should be accepted, we followed that recommendation. For the 150 papers where there was no clear consensus, at least one Program Chair closely examined the reviews, rebuttal, and discussion threads. In cases where a decision was still unclear, additional Program Chairs joined the discussion — always in a way that respected our own potential conflicts of interest with authors. In some cases, we also reached out to the review team for further input. In all cases, we made sure the decision was one we would personally stand behind.

- **Good teamwork**: Our team of Program Chairs spanned computer science, philosophy, and law, two continents, and both academia and industry, bringing a range of perspectives to our conversations. Despite our different backgrounds, we established common goals early on and were all committed to working towards these goals to support the continued growth and success of the conference. Being a Program Chair is a lot of work, but much more satisfying with a strong team you can rely on.

There are also aspects of the review process where we believe there is room for improvement:

- **HotCRP**: As in 2024, we used the HotCRP platform to manage the submission and review process. HotCRP includes some features that make it relatively easy to construct the final ACM proceedings compared with other platforms. However, we struggled with HotCRP’s limited functionality compared with other modern review platforms, as well as its light documentation of features. This resulted in a lot of additional overhead throughout the process, particularly in recruiting and registering Reviewers and ACs, setting up the review form, assigning papers, and facilitating discussions among review teams. We strongly recommend that FAccT switch to the use of an alternative review platform for future years.

- **Focus areas**: To facilitate paper assignment on HotCRP and help route papers to reviewers with the appropriate expertise, we used six “[focus areas](https://facctconference.org/2025/focusareas),” slightly revised from those used in 2024.  Each AC and Reviewer could select one or more focus areas reflecting their expertise, and the authors were asked to select one or more focus areas for their submission.  ACs and Reviewers were then able to bid on any paper tagged with their selected focus area(s). However, this system presented some challenges. Most notably, the split of papers among focus areas was quite uneven, with more than half of submitted papers using the “evaluations and evaluation practices” focus area, making the tag somewhat uninformative. We encourage future Program Chairs to explore alternative ways of ensuring that submitted papers are matched to ACs and Reviewers with the appropriate interests and expertise to provide fair and accurate reviews.

- **Ethics flags**: A total 45 papers were flagged by ACs or Reviewers as raising potential ethical concerns, most commonly due to the absence of any mention of ethics committee (e.g., IRB) review for studies involving human subjects. Each flagged paper was carefully reviewed by at least one Program Chair, and in some cases the issue was clarified or resolved during the rebuttal phase. For the future, we recommend encouraging Reviewers to raise ethical concerns directly in their reviews, so that authors have an opportunity to respond. It may also be helpful to provide guidance on ethical review processes, given that practices such as IRB approval vary across disciplines, institutions, and geographies. 

- **Formatting requirements**: We ran into several challenges related to the paper formatting templates currently used by FAccT. In line with ACM guidance, FAccT offers both LaTeX and Word templates. However, we found that many authors who used the Word template had difficulty understanding how to properly prepare their submission. If the Word template is used in future years, we recommend that the Program Chairs provide detailed guidance for Word users on how to ensure their papers meet the requirements outlined in the Call for Papers. There was also confusion around the requirement that papers be submitted for review in single-column format, while camera-ready papers appear in double-column format, a discrepancy we suggest revisiting. In total, 48 submissions were desk rejected for violations of formatting requirements or other policies outlined in the Call for Papers (e.g., breaches of anonymity).

- **The big tent**: Finally, while we aimed to promote the big tent vision of FAccT, there is certainly more room for improvement. We hope that future Program Chairs will explore additional ways to encourage researchers working on aspects of responsible, safe, ethical, and trustworthy computing that are less well represented at FAccT to participate in the conference.

In short, it has been an honor to serve as this year’s Program Chairs and to help advance FAccT’s mission of supporting interdisciplinary research that is both intellectually robust and inclusive. We hope that the reflections and lessons shared here will contribute to the continued growth and success of the conference in the future.

